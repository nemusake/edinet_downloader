"""
çµ±åˆè²¡å‹™ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚·ã‚¹ãƒ†ãƒ 
ä¼æ¥­é¸æŠã‹ã‚‰70é …ç›®ã®è²¡å‹™ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã¾ã§ä¸€å…ƒåŒ–
"""
import pandas as pd
import json
import logging
import os
import zipfile
import tempfile
import shutil
import re
from datetime import datetime, date, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any
import sys
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET

from edinet_client.api.client import EdinetAPIClient

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class FinancialDataExtractor:
    """çµ±åˆè²¡å‹™ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.client = EdinetAPIClient()
        self.edinet_list_df = None
        self.fund_list_df = None
        self.financial_mapping = {}
        self.load_company_lists()
        self.load_financial_mapping()
    
    def load_company_lists(self):
        """ä¼æ¥­ãƒ»ãƒ•ã‚¡ãƒ³ãƒ‰ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        try:
            # EDINETã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿
            edinet_files = list(Path('.').glob('list_edinetcode_*.csv'))
            if edinet_files:
                latest_edinet = max(edinet_files, key=lambda x: x.name)
                self.edinet_list_df = pd.read_csv(latest_edinet, encoding='utf-8-sig')
                logger.info(f"EDINETãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿: {latest_edinet.name} ({len(self.edinet_list_df)}ä»¶)")
            
            # ãƒ•ã‚¡ãƒ³ãƒ‰ã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿
            fund_files = list(Path('.').glob('list_fundcode_*.csv'))
            if fund_files:
                latest_fund = max(fund_files, key=lambda x: x.name)
                self.fund_list_df = pd.read_csv(latest_fund, encoding='utf-8-sig')
                logger.info(f"ãƒ•ã‚¡ãƒ³ãƒ‰ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿: {latest_fund.name} ({len(self.fund_list_df)}ä»¶)")
                
        except Exception as e:
            logger.error(f"ä¼æ¥­ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def load_financial_mapping(self):
        """70é …ç›®ã®è²¡å‹™æŒ‡æ¨™ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’èª­ã¿è¾¼ã¿"""
        try:
            mapping_files = list(Path('.').glob('xbrl_fin_metadata_*.csv'))
            if not mapping_files:
                logger.error("è²¡å‹™æŒ‡æ¨™ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return
            
            latest_mapping = max(mapping_files, key=lambda x: x.name)
            df = pd.read_csv(latest_mapping, encoding='utf-8-sig')
            
            # XBRLã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆåã¨é …ç›®ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
            for _, row in df.iterrows():
                if pd.notna(row['xbrl_element']) and row['xbrl_element'] != 'è¨ˆç®—é …ç›®':
                    element_patterns = self._generate_element_patterns(row['xbrl_element'])
                    self.financial_mapping[row['item_name_en']] = {
                        'japanese_name': row['item_name_jp'],
                        'xbrl_patterns': element_patterns,
                        'unit': row['unit'],
                        'importance': row['importance'],
                        'category': row['category']
                    }
            
            logger.info(f"è²¡å‹™æŒ‡æ¨™ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿: {len(self.financial_mapping)}é …ç›®")
            
        except Exception as e:
            logger.error(f"è²¡å‹™æŒ‡æ¨™ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _generate_element_patterns(self, base_element: str) -> List[str]:
        """XBRLã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆåã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆ"""
        patterns = [base_element]
        
        element_name = base_element.split(':')[-1]
        
        common_prefixes = [
            'jppfs_cor', 'jpcrp_cor', 'jpdei_cor', 'jpigp_cor',
            'us-gaap', 'ifrs', 'jpfr', 'jpcre', 'jpcrp'
        ]
        
        for prefix in common_prefixes:
            patterns.append(f"{prefix}:{element_name}")
        
        patterns.append(element_name)
        return patterns
    
    def display_company_selection_menu(self, 
                                      filter_by_sec_code: bool = True,
                                      limit: int = 20) -> Optional[Dict]:
        """ä¼æ¥­é¸æŠãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’è¡¨ç¤º"""
        if self.edinet_list_df is None:
            logger.error("EDINETãƒªã‚¹ãƒˆãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return None
        
        df = self.edinet_list_df.copy()
        
        if filter_by_sec_code:
            df = df[df['è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰'].notna() & (df['è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰'] != '')]
            logger.info(f"è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ä¿æœ‰ä¼æ¥­: {len(df)}ä»¶")
        
        if len(df) > limit:
            df_display = df.head(limit)
            logger.info(f"è¡¨ç¤ºåˆ¶é™ã«ã‚ˆã‚Šæœ€åˆã®{limit}ä»¶ã‚’è¡¨ç¤º")
        else:
            df_display = df
        
        print("\n=== ä¼æ¥­é¸æŠãƒ¡ãƒ‹ãƒ¥ãƒ¼ ===")
        print("ç•ªå· | EDINETã‚³ãƒ¼ãƒ‰ | è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ | ä¼æ¥­å")
        print("-" * 80)
        
        for i, (_, row) in enumerate(df_display.iterrows(), 1):
            edinet_code = row['EDINETã‚³ãƒ¼ãƒ‰']
            sec_code = row['è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰'] if pd.notna(row['è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰']) else 'ãªã—'
            company_name = row['æå‡ºè€…å'][:30]
            
            print(f"{i:3d}  | {edinet_code:10s} | {str(sec_code):8s} | {company_name}")
        
        print(f"\næ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³:")
        print(f"{len(df_display) + 1}: ä¼æ¥­åã§æ¤œç´¢")
        print(f"{len(df_display) + 2}: EDINETã‚³ãƒ¼ãƒ‰ã§æ¤œç´¢")
        print(f"{len(df_display) + 3}: è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã§æ¤œç´¢")
        print(f"\nç‰¹åˆ¥ã‚ªãƒ—ã‚·ãƒ§ãƒ³:")
        print(f"{len(df_display) + 4}: ğŸš€ å…¨éŠ˜æŸ„ä¸€æ‹¬å‡¦ç†ï¼ˆæœŸé–“æŒ‡å®šï¼‰")
        print("0: çµ‚äº†")
        
        while True:
            try:
                choice = input(f"\né¸æŠã—ã¦ãã ã•ã„ (1-{len(df_display) + 4}, 0): ").strip()
                
                if choice == '0':
                    return None
                
                choice_num = int(choice)
                
                if 1 <= choice_num <= len(df_display):
                    selected_row = df_display.iloc[choice_num - 1]
                    return selected_row.to_dict()
                elif choice_num == len(df_display) + 1:
                    return self._search_by_name(df)
                elif choice_num == len(df_display) + 2:
                    return self._search_by_edinet_code(df)
                elif choice_num == len(df_display) + 3:
                    return self._search_by_sec_code(df)
                elif choice_num == len(df_display) + 4:
                    # å…¨éŠ˜æŸ„ä¸€æ‹¬å‡¦ç†ã‚’ç¤ºã™ç‰¹åˆ¥ãªå€¤ã‚’è¿”ã™
                    return {"å…¨éŠ˜æŸ„ä¸€æ‹¬å‡¦ç†": True}
                else:
                    print("ç„¡åŠ¹ãªé¸æŠã§ã™ã€‚å†åº¦å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                    
            except ValueError:
                print("æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            except KeyboardInterrupt:
                print("\nå‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã—ãŸã€‚")
                return None
    
    def _search_by_name(self, df: pd.DataFrame) -> Optional[Dict]:
        """ä¼æ¥­åã§æ¤œç´¢"""
        search_term = input("ä¼æ¥­åã®ä¸€éƒ¨ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
        if not search_term:
            return None
        
        results = df[df['æå‡ºè€…å'].str.contains(search_term, na=False, case=False)]
        return self._select_from_search_results(results, f"'{search_term}'ã‚’å«ã‚€ä¼æ¥­")
    
    def _search_by_edinet_code(self, df: pd.DataFrame) -> Optional[Dict]:
        """EDINETã‚³ãƒ¼ãƒ‰ã§æ¤œç´¢"""
        edinet_code = input("EDINETã‚³ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip().upper()
        if not edinet_code:
            return None
        
        results = df[df['EDINETã‚³ãƒ¼ãƒ‰'] == edinet_code]
        return self._select_from_search_results(results, f"EDINETã‚³ãƒ¼ãƒ‰ '{edinet_code}'")
    
    def _search_by_sec_code(self, df: pd.DataFrame) -> Optional[Dict]:
        """è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã§æ¤œç´¢"""
        sec_code = input("è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
        if not sec_code:
            return None
        
        results = df[df['è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰'].astype(str) == sec_code]
        return self._select_from_search_results(results, f"è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ '{sec_code}'")
    
    def _select_from_search_results(self, results: pd.DataFrame, search_desc: str) -> Optional[Dict]:
        """æ¤œç´¢çµæœã‹ã‚‰é¸æŠ"""
        if len(results) == 0:
            print(f"{search_desc}ã«è©²å½“ã™ã‚‹ä¼æ¥­ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None
        
        if len(results) == 1:
            selected = results.iloc[0].to_dict()
            print(f"ä¼æ¥­ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: {selected['æå‡ºè€…å']}")
            return selected
        
        print(f"\n{search_desc}ã®æ¤œç´¢çµæœ ({len(results)}ä»¶):")
        print("ç•ªå· | EDINETã‚³ãƒ¼ãƒ‰ | è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ | ä¼æ¥­å")
        print("-" * 80)
        
        for i, (_, row) in enumerate(results.iterrows(), 1):
            edinet_code = row['EDINETã‚³ãƒ¼ãƒ‰']
            sec_code = row['è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰'] if pd.notna(row['è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰']) else 'ãªã—'
            company_name = row['æå‡ºè€…å'][:30]
            
            print(f"{i:3d}  | {edinet_code:10s} | {str(sec_code):8s} | {company_name}")
        
        while True:
            try:
                choice = input(f"\né¸æŠã—ã¦ãã ã•ã„ (1-{len(results)}, 0=æˆ»ã‚‹): ").strip()
                
                if choice == '0':
                    return None
                
                choice_num = int(choice)
                if 1 <= choice_num <= len(results):
                    return results.iloc[choice_num - 1].to_dict()
                else:
                    print("ç„¡åŠ¹ãªé¸æŠã§ã™ã€‚")
                    
            except ValueError:
                print("æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    
    def estimate_filing_period(self, company_info: Dict) -> List[date]:
        """æ±ºç®—æœŸã‹ã‚‰æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã®æå‡ºæ™‚æœŸã‚’æ¨å®š"""
        fiscal_year_end = company_info.get('æ±ºç®—æ—¥', '')
        if not fiscal_year_end:
            logger.warning("æ±ºç®—æ—¥æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæœŸé–“ã‚’ä½¿ç”¨ã—ã¾ã™")
            return self._get_default_search_dates()
        
        try:
            current_year = datetime.now().year
            
            if 'æœˆ' in fiscal_year_end and 'æ—¥' in fiscal_year_end:
                month_str = fiscal_year_end.split('æœˆ')[0]
                day_str = fiscal_year_end.split('æœˆ')[1].replace('æ—¥', '')
                
                fiscal_month = int(month_str)
                fiscal_day = int(day_str)
                
                potential_dates = []
                
                for year in [current_year, current_year - 1]:
                    try:
                        fiscal_date = date(year, fiscal_month, fiscal_day)
                        
                        filing_start = fiscal_date + timedelta(days=60)
                        filing_end = fiscal_date + timedelta(days=90)
                        
                        current_date = filing_start
                        while current_date <= filing_end and current_date <= date.today():
                            potential_dates.append(current_date)
                            current_date += timedelta(days=1)
                            
                    except ValueError:
                        continue
                
                return sorted(potential_dates, reverse=True)
                
        except Exception as e:
            logger.warning(f"æ±ºç®—æœŸè§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        return self._get_default_search_dates()
    
    def _get_default_search_dates(self) -> List[date]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æ¤œç´¢æœŸé–“ï¼ˆéå»60æ—¥ï¼‰"""
        dates = []
        for i in range(60):
            dates.append(date.today() - timedelta(days=i))
        return dates
    
    def find_company_documents(self, company_info: Dict, max_search_days: int = 60) -> List[Dict]:
        """ä¼æ¥­ã®æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã‚’åŠ¹ç‡çš„ã«æ¤œç´¢"""
        target_edinet_code = company_info['EDINETã‚³ãƒ¼ãƒ‰']
        company_name = company_info['æå‡ºè€…å']
        
        logger.info(f"=== {company_name} ã®æ–‡æ›¸æ¤œç´¢é–‹å§‹ ===")
        
        search_dates = self.estimate_filing_period(company_info)
        search_dates = search_dates[:max_search_days]
        
        logger.info(f"æ¤œç´¢æœŸé–“: {len(search_dates)}æ—¥é–“")
        
        found_documents = []
        
        for search_date in search_dates:
            try:
                logger.info(f"æ¤œç´¢ä¸­: {search_date}")
                
                documents = self.client.get_documents_list(search_date)
                securities_reports = self.client.filter_securities_reports(documents)
                
                for doc in securities_reports:
                    if doc.get("edinetCode") == target_edinet_code:
                        found_documents.append(doc)
                        logger.info(f"ç™ºè¦‹: {doc.get('docDescription')} (æå‡º: {doc.get('submitDateTime')})")
                        
                        if len(found_documents) >= 1:
                            logger.info("æœ€æ–°ã®æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã‚’ç™ºè¦‹ã—ãŸãŸã‚æ¤œç´¢ã‚’çµ‚äº†")
                            return found_documents
                
            except Exception as e:
                logger.warning(f"{search_date} ã®æ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        return found_documents
    
    def download_and_extract_xbrl(self, doc_id: str, output_dir: str = "temp_xbrl") -> str:
        """XBRLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦è§£å‡"""
        logger.info(f"æ–‡æ›¸ {doc_id} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
        
        zip_data = self.client.download_document(doc_id)
        
        output_path = Path(output_dir) / doc_id
        output_path.mkdir(parents=True, exist_ok=True)
        
        with tempfile.NamedTemporaryFile(suffix='.zip') as temp_zip:
            temp_zip.write(zip_data)
            temp_zip.flush()
            
            with zipfile.ZipFile(temp_zip.name, 'r') as zip_ref:
                zip_ref.extractall(output_path)
        
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ {output_path} ã«è§£å‡ã—ã¾ã—ãŸ")
        return str(output_path)
    
    def find_xbrl_files(self, extract_path: str) -> List[Dict[str, str]]:
        """XBRLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ã—ã¦è©³ç´°æƒ…å ±ã‚’å–å¾—"""
        xbrl_files = []
        
        for xbrl_file in Path(extract_path).glob("**/*.xbrl"):
            file_info = {
                'path': str(xbrl_file),
                'name': xbrl_file.name,
                'size': xbrl_file.stat().st_size,
                'type': self._classify_xbrl_file(xbrl_file.name)
            }
            
            # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ–‡æ›¸ã‚’å„ªå…ˆ
            if 'instance' in xbrl_file.name.lower() or 'jpcrp' in xbrl_file.name:
                xbrl_files.insert(0, file_info)
            else:
                xbrl_files.append(file_info)
        
        logger.info(f"XBRLãƒ•ã‚¡ã‚¤ãƒ«ãŒ {len(xbrl_files)} å€‹è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        for i, file_info in enumerate(xbrl_files):
            logger.info(f"  {i+1}. {file_info['name']} ({file_info['type']}, {file_info['size']:,} bytes)")
        
        return xbrl_files
    
    def _classify_xbrl_file(self, filename: str) -> str:
        """XBRLãƒ•ã‚¡ã‚¤ãƒ«ã®ç¨®é¡ã‚’åˆ†é¡"""
        filename_lower = filename.lower()
        
        if 'jpcrp' in filename_lower and 'instance' in filename_lower:
            return "æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ–‡æ›¸"
        elif 'jpcrp' in filename_lower:
            return "æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸é–¢é€£"
        elif 'jpdei' in filename_lower:
            return "åŸºæœ¬æƒ…å ±ã‚¿ã‚¯ã‚½ãƒãƒŸ"
        elif 'jpigp' in filename_lower:
            return "æ¥­ç¨®åˆ¥ã‚¿ã‚¯ã‚½ãƒãƒŸ"
        elif 'jppfs' in filename_lower:
            return "è²¡å‹™è«¸è¡¨ã‚¿ã‚¯ã‚½ãƒãƒŸ"
        elif 'instance' in filename_lower:
            return "ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ–‡æ›¸"
        elif 'taxonomy' in filename_lower or 'tax' in filename_lower:
            return "ã‚¿ã‚¯ã‚½ãƒãƒŸãƒ•ã‚¡ã‚¤ãƒ«"
        else:
            return "ãã®ä»–XBRLæ–‡æ›¸"
    
    def extract_financial_data(self, xbrl_file_info: Dict[str, str]) -> Dict[str, Any]:
        """XBRLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        xbrl_path = xbrl_file_info['path']
        file_name = xbrl_file_info['name']
        file_type = xbrl_file_info['type']
        
        logger.info(f"XBRLãƒ•ã‚¡ã‚¤ãƒ«è§£æä¸­: {file_name} ({file_type})")
        
        try:
            with open(xbrl_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            soup = BeautifulSoup(content, 'xml')
            
            extracted_data = {}
            found_count = 0
            
            for item_key, mapping_info in self.financial_mapping.items():
                value = self._find_element_value(soup, mapping_info['xbrl_patterns'])
                
                if value is not None:
                    extracted_data[item_key] = {
                        'value': value,
                        'japanese_name': mapping_info['japanese_name'],
                        'unit': mapping_info['unit'],
                        'importance': mapping_info['importance'],
                        'category': mapping_info['category']
                    }
                    found_count += 1
                else:
                    extracted_data[item_key] = {
                        'value': None,
                        'japanese_name': mapping_info['japanese_name'],
                        'unit': mapping_info['unit'],
                        'importance': mapping_info['importance'],
                        'category': mapping_info['category']
                    }
            
            logger.info(f"è²¡å‹™ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†: {found_count}/{len(self.financial_mapping)}é …ç›®")
            
            period_info = self._extract_period_info(soup)
            
            return {
                'financial_data': extracted_data,
                'period_info': period_info,
                'source_file_info': {
                    'filename': file_name,
                    'file_type': file_type,
                    'file_size': xbrl_file_info['size'],
                    'extraction_datetime': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                },
                'extraction_summary': {
                    'total_items': len(self.financial_mapping),
                    'found_items': found_count,
                    'success_rate': round(found_count / len(self.financial_mapping) * 100, 1)
                }
            }
            
        except Exception as e:
            logger.error(f"XBRLè§£æã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def _find_element_value(self, soup: BeautifulSoup, patterns: List[str]) -> Optional[str]:
        """XBRLã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã®å€¤ã‚’æ¤œç´¢"""
        for pattern in patterns:
            elements = soup.find_all(pattern)
            if elements:
                return self._extract_numeric_value(elements[0])
            
            if ':' in pattern:
                local_name = pattern.split(':')[-1]
                elements = soup.find_all(lambda tag: tag.name and tag.name.endswith(local_name))
                if elements:
                    return self._extract_numeric_value(elements[0])
            
            elements = soup.find_all(lambda tag: tag.name and tag.name.lower() == pattern.lower())
            if elements:
                return self._extract_numeric_value(elements[0])
        
        return None
    
    def _extract_numeric_value(self, element) -> Optional[str]:
        """ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º"""
        if element is None:
            return None
        
        text = element.get_text(strip=True)
        
        if not text:
            return None
        
        numeric_text = re.sub(r'[^\d.-]', '', text)
        
        if numeric_text:
            try:
                float(numeric_text)
                return numeric_text
            except ValueError:
                pass
        
        return text
    
    def _extract_period_info(self, soup: BeautifulSoup) -> Dict[str, str]:
        """æœŸé–“æƒ…å ±ã‚’æŠ½å‡º"""
        period_info = {}
        
        period_patterns = [
            ('period_start', ['periodStart', 'PeriodStart', 'startDate']),
            ('period_end', ['periodEnd', 'PeriodEnd', 'endDate']),
            ('instant', ['instant', 'Instant'])
        ]
        
        for key, patterns in period_patterns:
            for pattern in patterns:
                elements = soup.find_all(lambda tag: tag.name and pattern.lower() in tag.name.lower())
                if elements:
                    period_info[key] = elements[0].get_text(strip=True)
                    break
        
        return period_info
    
    def _extract_document_date_and_name(self, doc_info: Dict) -> tuple:
        """æ–‡æ›¸ã®å…¬é–‹æ—¥ä»˜ã¨æ›¸é¡åç§°ã‚’æŠ½å‡º"""
        # å…¬é–‹æ—¥ä»˜ã®æŠ½å‡ºï¼ˆæå‡ºæ—¥æ™‚ã‹ã‚‰æ—¥ä»˜éƒ¨åˆ†ã‚’å–å¾—ï¼‰
        submit_datetime = doc_info.get('submitDateTime', '')
        if submit_datetime:
            try:
                # "2025-06-26 13:21" -> "2025-06-26"
                date_part = submit_datetime.split(' ')[0]
                return date_part, self._classify_document_type(doc_info)
            except:
                pass
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæœŸæœ«æ—¥ã‚’ä½¿ç”¨
        period_end = doc_info.get('periodEnd', '')
        if period_end:
            return period_end, self._classify_document_type(doc_info)
        
        return 'unknown', self._classify_document_type(doc_info)
    
    def _classify_document_type(self, doc_info: Dict) -> str:
        """æ–‡æ›¸ç¨®åˆ¥ã‚’åˆ†é¡"""
        doc_description = doc_info.get('docDescription', '').lower()
        form_code = doc_info.get('formCode', '')
        
        # æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸
        if 'æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸' in doc_description or form_code == '030000':
            return 'æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸'
        # å››åŠæœŸå ±å‘Šæ›¸
        elif 'å››åŠæœŸå ±å‘Šæ›¸' in doc_description or form_code in ['043000', '044000']:
            return 'å››åŠæœŸå ±å‘Šæ›¸'
        # åŠæœŸå ±å‘Šæ›¸  
        elif 'åŠæœŸå ±å‘Šæ›¸' in doc_description or form_code == '050000':
            return 'åŠæœŸå ±å‘Šæ›¸'
        # è‡¨æ™‚å ±å‘Šæ›¸
        elif 'è‡¨æ™‚å ±å‘Šæ›¸' in doc_description or form_code == '070000':
            return 'è‡¨æ™‚å ±å‘Šæ›¸'
        # æœ‰ä¾¡è¨¼åˆ¸å±Šå‡ºæ›¸
        elif 'æœ‰ä¾¡è¨¼åˆ¸å±Šå‡ºæ›¸' in doc_description:
            return 'æœ‰ä¾¡è¨¼åˆ¸å±Šå‡ºæ›¸'
        # å¤‰æ›´å ±å‘Šæ›¸
        elif 'å¤‰æ›´å ±å‘Šæ›¸' in doc_description:
            return 'å¤‰æ›´å ±å‘Šæ›¸'
        # è¨‚æ­£å ±å‘Šæ›¸
        elif 'è¨‚æ­£' in doc_description:
            if 'æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸' in doc_description:
                return 'æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ï¼ˆè¨‚æ­£ï¼‰'
            elif 'å››åŠæœŸå ±å‘Šæ›¸' in doc_description:
                return 'å››åŠæœŸå ±å‘Šæ›¸ï¼ˆè¨‚æ­£ï¼‰'
            elif 'åŠæœŸå ±å‘Šæ›¸' in doc_description:
                return 'åŠæœŸå ±å‘Šæ›¸ï¼ˆè¨‚æ­£ï¼‰'
            else:
                return 'è¨‚æ­£å ±å‘Šæ›¸'
        else:
            # form_codeã‹ã‚‰æ¨å®š
            form_code_mapping = {
                '010000': 'å±Šå‡ºæ›¸',
                '020000': 'ç›®è«–è¦‹æ›¸',
                '030000': 'æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸',
                '040000': 'å››åŠæœŸå ±å‘Šæ›¸',
                '050000': 'åŠæœŸå ±å‘Šæ›¸',
                '060000': 'è‡¨æ™‚å ±å‘Šæ›¸',
                '070000': 'è‡¨æ™‚å ±å‘Šæ›¸',
                '080000': 'è¦ªä¼šç¤¾ç­‰çŠ¶æ³å ±å‘Šæ›¸',
                '090000': 'è‡ªå·±æ ªåˆ¸è²·ä»˜çŠ¶æ³å ±å‘Šæ›¸',
                '100000': 'å¤‰æ›´å ±å‘Šæ›¸',
                '110000': 'è¨‚æ­£å±Šå‡ºæ›¸',
                '120000': 'æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸',
                '130000': 'æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ï¼ˆè¨‚æ­£ï¼‰'
            }
            return form_code_mapping.get(form_code, 'ãã®ä»–æ›¸é¡')

    def save_extracted_data(self, extracted_data: Dict, company_info: Dict, doc_info: Dict) -> str:
        """æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        company_safe_name = company_info.get('æå‡ºè€…å', 'unknown').replace('æ ªå¼ä¼šç¤¾', '').replace(' ', '_')[:10]
        
        # æ–‡æ›¸ã®æ—¥ä»˜ã¨åç§°ã‚’å–å¾—
        doc_date, doc_name = self._extract_document_date_and_name(doc_info)
        
        output_data = {
            'extraction_timestamp': timestamp,
            'company_info': company_info,
            'document_info': doc_info,
            'extracted_data': extracted_data
        }
        
        json_file = f"{company_safe_name}_financial_data_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        csv_data = []
        if 'financial_data' in extracted_data:
            for item_key, item_data in extracted_data['financial_data'].items():
                csv_data.append({
                    'date': doc_date,
                    'doc_name': doc_name,
                    'item_key': item_key,
                    'japanese_name': item_data['japanese_name'],
                    'value': item_data['value'],
                    'unit': item_data['unit'],
                    'importance': item_data['importance'],
                    'category': item_data['category']
                })
        
        if csv_data:
            csv_file = f"{company_safe_name}_financial_data_{timestamp}.csv"
            df = pd.DataFrame(csv_data)
            # ã‚«ãƒ©ãƒ é †åºã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
            column_order = ['date', 'doc_name', 'item_key', 'japanese_name', 'value', 'unit', 'importance', 'category']
            df = df[column_order]
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            
            logger.info(f"CSVä¿å­˜å®Œäº†: {len(csv_data)}è¡Œã®ãƒ‡ãƒ¼ã‚¿")
        
        logger.info(f"æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {json_file}")
        return json_file
    
    def _ensure_output_directories(self) -> tuple[Path, Path]:
        """output/csvã¨output/jsonãƒ•ã‚©ãƒ«ãƒ€ã®å­˜åœ¨ç¢ºèªã¨è‡ªå‹•ä½œæˆ"""
        output_dir = Path('output')
        csv_dir = output_dir / 'csv'
        json_dir = output_dir / 'json'
        
        # å„ãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆ
        for dir_path in [output_dir, csv_dir, json_dir]:
            if not dir_path.exists():
                dir_path.mkdir(parents=True, exist_ok=True)
                logger.info(f"ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¾ã—ãŸ: {dir_path.absolute()}")
        
        return csv_dir, json_dir
    
    def save_document_individual(self, extracted_data: Dict, company_info: Dict, doc_info: Dict) -> str:
        """å˜ä¸€æ–‡æ›¸ã‚’å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆCSV/JSONåˆ¥ãƒ•ã‚©ãƒ«ãƒ€ï¼‰"""
        # output/csvã¨output/jsonãƒ•ã‚©ãƒ«ãƒ€ã®ç¢ºä¿
        csv_dir, json_dir = self._ensure_output_directories()
        
        # ä¼æ¥­æƒ…å ±ã‹ã‚‰å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åè¦ç´ ã‚’ç”Ÿæˆ
        company_name = company_info.get('æå‡ºè€…å', 'unknown').replace('æ ªå¼ä¼šç¤¾', '').replace(' ', '').replace('ã€€', '')[:10]
        securities_code = company_info.get('è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰', 'nocode')
        edinet_code = company_info.get('EDINETã‚³ãƒ¼ãƒ‰', 'noedinet')
        
        # æ–‡æ›¸æƒ…å ±ã‹ã‚‰æ—¥ä»˜ã¨æ›¸é¡ç¨®åˆ¥ã‚’å–å¾—
        doc_date, doc_name = self._extract_document_date_and_name(doc_info)
        
        # æ—¥ä»˜ã‚’yyyymmddå½¢å¼ã«å¤‰æ›
        try:
            if doc_date and '-' in doc_date:
                date_str = doc_date.replace('-', '')
            else:
                date_str = datetime.now().strftime('%Y%m%d')
        except:
            date_str = datetime.now().strftime('%Y%m%d')
        
        # æ›¸é¡ç¨®åˆ¥ã‚’ãƒ•ã‚¡ã‚¤ãƒ«åç”¨ã«èª¿æ•´
        doc_name_safe = doc_name.replace('ï¼ˆ', '').replace('ï¼‰', '').replace('ãƒ»', '').replace(' ', '')
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ: ä¼æ¥­å_è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰_EDINETã‚³ãƒ¼ãƒ‰_æ›¸é¡ç¨®åˆ¥_yyyymmdd
        base_filename = f"{company_name}_{securities_code}_{edinet_code}_{doc_name_safe}_{date_str}"
        
        # JSONä¿å­˜ï¼ˆoutput/jsonãƒ•ã‚©ãƒ«ãƒ€å†…ï¼‰
        output_data = {
            'extraction_date': date_str,
            'company_info': company_info,
            'document_info': doc_info,
            'extracted_data': extracted_data
        }
        
        json_file_path = json_dir / f"{base_filename}.json"
        with open(json_file_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        # CSVä¿å­˜ï¼ˆoutput/csvãƒ•ã‚©ãƒ«ãƒ€å†…ï¼‰
        csv_data = []
        if 'financial_data' in extracted_data:
            for item_key, item_data in extracted_data['financial_data'].items():
                csv_data.append({
                    'date': doc_date,
                    'doc_name': doc_name,
                    'item_key': item_key,
                    'japanese_name': item_data['japanese_name'],
                    'value': item_data['value'],
                    'unit': item_data['unit'],
                    'importance': item_data['importance'],
                    'category': item_data['category']
                })
        
        if csv_data:
            csv_file_path = csv_dir / f"{base_filename}.csv"
            df = pd.DataFrame(csv_data)
            # ã‚«ãƒ©ãƒ é †åºã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
            column_order = ['date', 'doc_name', 'item_key', 'japanese_name', 'value', 'unit', 'importance', 'category']
            df = df[column_order]
            df.to_csv(csv_file_path, index=False, encoding='utf-8-sig')
            
            logger.info(f"å€‹åˆ¥CSVä¿å­˜å®Œäº†: {csv_file_path} ({len(csv_data)}è¡Œ)")
        
        logger.info(f"å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: output/csv/{base_filename}.csv ã¨ output/json/{base_filename}.json")
        return str(json_file_path)
    
    def process_company_document(self, company_info: Dict, doc_info: Dict) -> Optional[str]:
        """ä¼æ¥­æ–‡æ›¸ã®å‡¦ç†ï¼ˆå®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼‰"""
        doc_id = doc_info['docID']
        company_name = company_info.get('æå‡ºè€…å', 'unknown')
        
        logger.info(f"=== {company_name} ã®è²¡å‹™ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºé–‹å§‹ ===")
        logger.info(f"æ–‡æ›¸ID: {doc_id}")
        logger.info(f"æå‡ºæ—¥æ™‚: {doc_info.get('submitDateTime', 'ä¸æ˜')}")
        logger.info(f"å¯¾è±¡æœŸé–“: {doc_info.get('periodStart', 'ä¸æ˜')} ï½ {doc_info.get('periodEnd', 'ä¸æ˜')}")
        
        try:
            extract_path = self.download_and_extract_xbrl(doc_id)
            xbrl_files = self.find_xbrl_files(extract_path)
            
            if not xbrl_files:
                logger.error("XBRLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return None
            
            print(f"\n=== åˆ©ç”¨å¯èƒ½ãªXBRLãƒ•ã‚¡ã‚¤ãƒ« ===")
            for i, file_info in enumerate(xbrl_files):
                print(f"{i+1}. {file_info['name']}")
                print(f"   ç¨®é¡: {file_info['type']}")
                print(f"   ã‚µã‚¤ã‚º: {file_info['size']:,} bytes")
            
            # ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‚’ä½¿ç”¨
            main_xbrl = xbrl_files[0]
            print(f"\n--- ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {main_xbrl['name']} ---")
            
            extracted_data = self.extract_financial_data(main_xbrl)
            
            if not extracted_data:
                logger.error("è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
                return None
            
            # XBRLãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚‚ä¿å­˜ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
            extracted_data['available_xbrl_files'] = xbrl_files
            
            output_file = self.save_extracted_data(extracted_data, company_info, doc_info)
            
            shutil.rmtree(extract_path, ignore_errors=True)
            
            return output_file
            
        except Exception as e:
            logger.error(f"æ–‡æ›¸å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def display_extraction_summary(self, result_file: str):
        """æŠ½å‡ºçµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        try:
            with open(result_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            extracted_data = data['extracted_data']
            summary = extracted_data.get('extraction_summary', {})
            financial_data = extracted_data.get('financial_data', {})
            source_info = extracted_data.get('source_file_info', {})
            
            print(f"\n=== è²¡å‹™ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚µãƒãƒªãƒ¼ ===")
            print(f"å¯¾è±¡é …ç›®æ•°: {summary.get('total_items', 0)}")
            print(f"æŠ½å‡ºæˆåŠŸ: {summary.get('found_items', 0)}")
            print(f"æˆåŠŸç‡: {summary.get('success_rate', 0)}%")
            
            # æ–‡æ›¸æƒ…å ±
            doc_info = data.get('document_info', {})
            if doc_info:
                doc_date, doc_name = self._extract_document_date_and_name(doc_info)
                print(f"\n=== å¯¾è±¡æ–‡æ›¸æƒ…å ± ===")
                print(f"æ›¸é¡å: {doc_name}")
                print(f"å…¬é–‹æ—¥: {doc_date}")
                print(f"æ–‡æ›¸: {doc_info.get('docDescription', 'ä¸æ˜')}")
                print(f"å¯¾è±¡æœŸé–“: {doc_info.get('periodStart', 'ä¸æ˜')} ï½ {doc_info.get('periodEnd', 'ä¸æ˜')}")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æƒ…å ±
            if source_info:
                print(f"\n=== ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æƒ…å ± ===")
                print(f"ä½¿ç”¨ãƒ•ã‚¡ã‚¤ãƒ«: {source_info.get('filename', 'ä¸æ˜')}")
                print(f"ãƒ•ã‚¡ã‚¤ãƒ«ç¨®é¡: {source_info.get('file_type', 'ä¸æ˜')}")
                print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {source_info.get('file_size', 0):,} bytes")
                print(f"æŠ½å‡ºæ—¥æ™‚: {source_info.get('extraction_datetime', 'ä¸æ˜')}")
            
            # é‡è¦åº¦åˆ¥çµ±è¨ˆ
            importance_stats = {}
            category_stats = {}
            successful_items = []
            
            for item_key, item_data in financial_data.items():
                importance = item_data.get('importance', 'unknown')
                if importance not in importance_stats:
                    importance_stats[importance] = {'total': 0, 'found': 0}
                importance_stats[importance]['total'] += 1
                
                category = item_data.get('category', 'unknown')
                if category not in category_stats:
                    category_stats[category] = {'total': 0, 'found': 0}
                category_stats[category]['total'] += 1
                
                if item_data.get('value') is not None:
                    importance_stats[importance]['found'] += 1
                    category_stats[category]['found'] += 1
                    successful_items.append(item_data)
            
            print(f"\n--- é‡è¦åº¦åˆ¥æŠ½å‡ºçŠ¶æ³ ---")
            for importance, stats in importance_stats.items():
                rate = round(stats['found'] / stats['total'] * 100, 1) if stats['total'] > 0 else 0
                print(f"{importance}: {stats['found']}/{stats['total']} ({rate}%)")
            
            print(f"\n--- ã‚«ãƒ†ã‚´ãƒªåˆ¥æŠ½å‡ºçŠ¶æ³ ---")
            for category, stats in category_stats.items():
                rate = round(stats['found'] / stats['total'] * 100, 1) if stats['total'] > 0 else 0
                print(f"{category}: {stats['found']}/{stats['total']} ({rate}%)")
            
            if successful_items:
                print(f"\n--- æŠ½å‡ºæˆåŠŸä¾‹ï¼ˆæœ€åˆã®10é …ç›®ï¼‰ ---")
                for i, item in enumerate(successful_items[:10]):
                    value = item['value']
                    unit = item.get('unit', '')
                    name = item['japanese_name']
                    print(f"{i+1:2d}. {name}: {value} {unit}")
            
        except Exception as e:
            print(f"ã‚µãƒãƒªãƒ¼è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
    
    def process_date_range(self, company_info: Dict, start_date: str, end_date: str) -> List[str]:
        """æ—¥ä»˜ç¯„å›²æŒ‡å®šã§ã®è¤‡æ•°æ–‡æ›¸å‡¦ç†"""
        company_name = company_info.get('æå‡ºè€…å', 'unknown')
        edinet_code = company_info.get('EDINETã‚³ãƒ¼ãƒ‰', 'unknown')
        
        print(f"\n=== {company_name} ã®æ—¥ä»˜ç¯„å›²å‡¦ç† ===")
        print(f"æœŸé–“: {start_date} ï½ {end_date}")
        print(f"EDINETã‚³ãƒ¼ãƒ‰: {edinet_code}")
        
        try:
            start_dt = datetime.strptime(start_date, '%Y-%m-%d').date()
            end_dt = datetime.strptime(end_date, '%Y-%m-%d').date()
        except ValueError as e:
            print(f"âŒ æ—¥ä»˜å½¢å¼ã‚¨ãƒ©ãƒ¼: {e}")
            return []
        
        # æ—¥ä»˜ç¯„å›²ã§ã®æ–‡æ›¸æ¤œç´¢
        all_documents = []
        current_date = start_dt
        
        while current_date <= end_dt:
            try:
                print(f"æ¤œç´¢ä¸­: {current_date}")
                documents = self.client.get_documents_list(current_date)
                securities_reports = self.client.filter_securities_reports(documents)
                
                # å¯¾è±¡ä¼æ¥­ã®æ–‡æ›¸ã®ã¿æŠ½å‡º
                for doc in securities_reports:
                    if doc.get("edinetCode") == edinet_code:
                        doc['search_date'] = str(current_date)  # æ¤œç´¢æ—¥ã‚’è¨˜éŒ²
                        doc_name = self._classify_document_type(doc)
                        all_documents.append(doc)
                        print(f"  ç™ºè¦‹: {doc_name} - {doc.get('docDescription')} (æå‡º: {doc.get('submitDateTime')})")
                
            except Exception as e:
                print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
            
            current_date += timedelta(days=1)
        
        print(f"\nåˆè¨ˆ {len(all_documents)} ä»¶ã®æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        
        if not all_documents:
            print("å¯¾è±¡æœŸé–“ã«æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return []
        
        # å„æ–‡æ›¸ã‚’å‡¦ç†
        result_files = []
        for i, doc in enumerate(all_documents, 1):
            print(f"\n--- æ–‡æ›¸ {i}/{len(all_documents)} ã®å‡¦ç† ---")
            print(f"æ–‡æ›¸: {doc.get('docDescription')}")
            print(f"æ¤œç´¢æ—¥: {doc.get('search_date')}")
            
            result_file = self.process_company_document(company_info, doc)
            if result_file:
                result_files.append(result_file)
                print(f"âœ… å‡¦ç†å®Œäº†: {result_file}")
            else:
                print(f"âŒ å‡¦ç†å¤±æ•—")
        
        return result_files
    
    def process_date_range_individual(self, company_info: Dict, start_date: str, end_date: str) -> List[str]:
        """æ—¥ä»˜ç¯„å›²å†…ã®æ–‡æ›¸ã‚’å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã§å‡¦ç†"""
        edinet_code = company_info.get('EDINETã‚³ãƒ¼ãƒ‰')
        company_name = company_info.get('æå‡ºè€…å')
        
        logger.info(f"=== {company_name} ã®æœŸé–“å†…æ–‡æ›¸å€‹åˆ¥å‡¦ç†é–‹å§‹ ===")
        logger.info(f"æœŸé–“: {start_date} ï½ {end_date}")
        
        # æ—¥ä»˜ç¯„å›²ã‚’è¨­å®š
        current_date = datetime.strptime(start_date, '%Y-%m-%d').date()
        end_date_obj = datetime.strptime(end_date, '%Y-%m-%d').date()
        
        all_documents = []
        
        print(f"\n--- æœŸé–“å†…æ–‡æ›¸æ¤œç´¢: {start_date} ï½ {end_date} ---")
        
        while current_date <= end_date_obj:
            print(f"æ¤œç´¢ä¸­: {current_date}")
            
            try:
                documents = self.client.get_documents_list(current_date)
                securities_reports = self.client.filter_securities_reports(documents)
                
                # å¯¾è±¡ä¼æ¥­ã®æ–‡æ›¸ã®ã¿æŠ½å‡º
                for doc in securities_reports:
                    if doc.get("edinetCode") == edinet_code:
                        doc['search_date'] = str(current_date)  # æ¤œç´¢æ—¥ã‚’è¨˜éŒ²
                        doc_name = self._classify_document_type(doc)
                        all_documents.append(doc)
                        print(f"  ç™ºè¦‹: {doc_name} - {doc.get('docDescription')} (æå‡º: {doc.get('submitDateTime')})")
                
            except Exception as e:
                print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
            
            current_date += timedelta(days=1)
        
        print(f"\nåˆè¨ˆ {len(all_documents)} ä»¶ã®æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        
        if not all_documents:
            print("å¯¾è±¡æœŸé–“ã«æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return []
        
        # å„æ–‡æ›¸ã‚’å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã§å‡¦ç†
        result_files = []
        for i, doc in enumerate(all_documents, 1):
            print(f"\n--- æ–‡æ›¸ {i}/{len(all_documents)} ã®å€‹åˆ¥å‡¦ç† ---")
            print(f"æ–‡æ›¸: {doc.get('docDescription')}")
            print(f"æ¤œç´¢æ—¥: {doc.get('search_date')}")
            
            try:
                # XBRLãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨è§£æ
                doc_id = doc['docID']
                extract_path = self.download_and_extract_xbrl(doc_id)
                xbrl_files = self.find_xbrl_files(extract_path)
                
                if not xbrl_files:
                    print("âŒ XBRLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    continue
                
                # ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
                main_xbrl = xbrl_files[0]
                extracted_data = self.extract_financial_data(main_xbrl)
                
                if not extracted_data:
                    print("âŒ è²¡å‹™ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
                    continue
                
                # å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã§ä¿å­˜
                result_file = self.save_document_individual(extracted_data, company_info, doc)
                result_files.append(result_file)
                print(f"âœ… å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†: {result_file}")
                
            except Exception as e:
                print(f"âŒ å‡¦ç†å¤±æ•—: {e}")
                logger.error(f"æ–‡æ›¸å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        
        return result_files
    
    def process_all_companies_in_period(self, start_date: str, end_date: str) -> List[str]:
        """å…¨éŠ˜æŸ„ã®æœŸé–“å†…æ–‡æ›¸ã‚’ä¸€æ‹¬å‡¦ç†"""
        logger.info(f"=== å…¨éŠ˜æŸ„ä¸€æ‹¬å‡¦ç†é–‹å§‹ ===")
        logger.info(f"æœŸé–“: {start_date} ï½ {end_date}")
        
        # æ—¥ä»˜ç¯„å›²ã‚’è¨­å®š
        current_date = datetime.strptime(start_date, '%Y-%m-%d').date()
        end_date_obj = datetime.strptime(end_date, '%Y-%m-%d').date()
        
        all_result_files = []
        processed_count = 0
        error_count = 0
        
        print(f"\n--- å…¨éŠ˜æŸ„æœŸé–“å†…æ–‡æ›¸ä¸€æ‹¬å‡¦ç†: {start_date} ï½ {end_date} ---")
        print("â€» ã“ã®å‡¦ç†ã¯éå¸¸ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼ˆæ•°æ™‚é–“ã€œæ•°æ—¥ï¼‰")
        print("â€» 11,075ç¤¾ Ã— æœŸé–“å†…æ–‡æ›¸æ•°ã®å‡¦ç†ãŒå®Ÿè¡Œã•ã‚Œã¾ã™")
        
        while current_date <= end_date_obj:
            print(f"\nğŸ” æ—¥ä»˜: {current_date} ã®æ–‡æ›¸æ¤œç´¢ä¸­...")
            
            try:
                # ãã®æ—¥ã®å…¨æ–‡æ›¸ã‚’å–å¾—
                documents = self.client.get_documents_list(current_date)
                securities_reports = self.client.filter_securities_reports(documents)
                
                print(f"  ç™ºè¦‹ã•ã‚ŒãŸæ–‡æ›¸æ•°: {len(securities_reports)}ä»¶")
                
                # å„æ–‡æ›¸ã‚’å‡¦ç†
                for i, doc in enumerate(securities_reports, 1):
                    edinet_code = doc.get("edinetCode")
                    doc_description = doc.get("docDescription", "")
                    
                    print(f"  å‡¦ç†ä¸­ ({i}/{len(securities_reports)}): {doc_description[:50]}...")
                    
                    try:
                        # EDINETã‚³ãƒ¼ãƒ‰ã‹ã‚‰ä¼æ¥­æƒ…å ±ã‚’å–å¾—
                        company_info = self._get_company_info_by_edinet_code(edinet_code)
                        if not company_info:
                            print(f"    âŒ ä¼æ¥­æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {edinet_code}")
                            error_count += 1
                            continue
                        
                        # XBRLãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨è§£æ
                        doc_id = doc['docID']
                        extract_path = self.download_and_extract_xbrl(doc_id)
                        xbrl_files = self.find_xbrl_files(extract_path)
                        
                        if not xbrl_files:
                            print(f"    âŒ XBRLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                            error_count += 1
                            continue
                        
                        # ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
                        main_xbrl = xbrl_files[0]
                        extracted_data = self.extract_financial_data(main_xbrl)
                        
                        if not extracted_data:
                            print(f"    âŒ è²¡å‹™ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã«å¤±æ•—")
                            error_count += 1
                            continue
                        
                        # å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã§ä¿å­˜
                        result_file = self.save_document_individual(extracted_data, company_info, doc)
                        all_result_files.append(result_file)
                        processed_count += 1
                        
                        print(f"    âœ… å‡¦ç†å®Œäº†: {company_info.get('æå‡ºè€…å', 'unknown')}")
                        
                    except Exception as e:
                        print(f"    âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                        logger.error(f"æ–‡æ›¸å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                        error_count += 1
                        
                    # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
                    import time
                    time.sleep(1)
                
            except Exception as e:
                print(f"  æ—¥ä»˜å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                logger.error(f"æ—¥ä»˜å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            
            current_date += timedelta(days=1)
        
        print(f"\n=== å…¨éŠ˜æŸ„ä¸€æ‹¬å‡¦ç†å®Œäº† ===")
        print(f"å‡¦ç†æˆåŠŸ: {processed_count}ä»¶")
        print(f"å‡¦ç†å¤±æ•—: {error_count}ä»¶")
        print(f"å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(all_result_files) * 2}ä»¶ï¼ˆCSV/JSONå„{len(all_result_files)}ä»¶ï¼‰")
        print(f"CSVå‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€: ./output/csv/")
        print(f"JSONå‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€: ./output/json/")
        
        return all_result_files
    
    def _get_company_info_by_edinet_code(self, edinet_code: str) -> Optional[Dict]:
        """EDINETã‚³ãƒ¼ãƒ‰ã‹ã‚‰ä¼æ¥­æƒ…å ±ã‚’å–å¾—"""
        if self.edinet_list_df is None:
            return None
        
        results = self.edinet_list_df[self.edinet_list_df['EDINETã‚³ãƒ¼ãƒ‰'] == edinet_code]
        if len(results) == 0:
            return None
        
        return results.iloc[0].to_dict()
    
    def run_interactive_mode(self):
        """å¯¾è©±å‹ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        print("=== çµ±åˆè²¡å‹™ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚·ã‚¹ãƒ†ãƒ  ===")
        print("ä¼æ¥­ã‚’é¸æŠã—ã¦70é …ç›®ã®è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•å–å¾—ã—ã¾ã™")
        
        while True:
            try:
                # 1. ä¼æ¥­é¸æŠ
                print("\n" + "="*50)
                company_info = self.display_company_selection_menu()
                
                if company_info is None:
                    print("ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                    break
                
                # å…¨éŠ˜æŸ„ä¸€æ‹¬å‡¦ç†ã®åˆ¤å®š
                if company_info.get('å…¨éŠ˜æŸ„ä¸€æ‹¬å‡¦ç†'):
                    print(f"\n=== ğŸš€ å…¨éŠ˜æŸ„ä¸€æ‹¬å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ ===")
                    print("æœŸé–“å†…ã®å…¨ä¼æ¥­ï¼ˆ11,075ç¤¾ï¼‰ã®è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬å–å¾—ã—ã¾ã™")
                    print("â€» ã“ã®å‡¦ç†ã¯éå¸¸ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼ˆæ•°æ™‚é–“ã€œæ•°æ—¥ï¼‰")
                    print("â€» CSVå‡ºåŠ›å…ˆ: ./output/csv/ ãƒ•ã‚©ãƒ«ãƒ€")
                    print("â€» JSONå‡ºåŠ›å…ˆ: ./output/json/ ãƒ•ã‚©ãƒ«ãƒ€")
                    print("â€» ãƒ•ã‚¡ã‚¤ãƒ«å: ä¼æ¥­å_è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰_EDINETã‚³ãƒ¼ãƒ‰_æ›¸é¡ç¨®åˆ¥_yyyymmdd")
                    
                    # æœŸé–“æŒ‡å®š
                    print(f"\næ—¥ä»˜å½¢å¼: YYYY-MM-DDï¼ˆä¾‹: 2024-01-01ï¼‰")
                    start_date = input("é–‹å§‹æ—¥ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
                    end_date = input("çµ‚äº†æ—¥ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
                    
                    if not start_date or not end_date:
                        print("æ—¥ä»˜ãŒå…¥åŠ›ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                        continue
                    
                    print(f"\nâš ï¸  è­¦å‘Š: {start_date} ï½ {end_date} ã®æœŸé–“ã§å…¨éŠ˜æŸ„å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™")
                    print("ã“ã®å‡¦ç†ã¯éå¸¸ã«æ™‚é–“ãŒã‹ã‹ã‚Šã€å¤§é‡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã™")
                    confirm = input("æœ¬å½“ã«å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ (yes/no): ").strip().lower()
                    
                    if confirm != 'yes':
                        print("å‡¦ç†ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸã€‚")
                        continue
                    
                    # å…¨éŠ˜æŸ„ä¸€æ‹¬å‡¦ç†å®Ÿè¡Œ
                    result_files = self.process_all_companies_in_period(start_date, end_date)
                    
                    if result_files:
                        print(f"\nğŸ‰ å…¨éŠ˜æŸ„ä¸€æ‹¬å‡¦ç†å®Œäº†!")
                        print(f"å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(result_files) * 2}ä»¶ï¼ˆCSV: {len(result_files)}ä»¶, JSON: {len(result_files)}ä»¶ï¼‰")
                        print("CSVãƒ•ã‚¡ã‚¤ãƒ«: ./output/csv/ ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜")
                        print("JSONãƒ•ã‚¡ã‚¤ãƒ«: ./output/json/ ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜")
                    else:
                        print("âŒ å‡¦ç†ã§ããŸæ–‡æ›¸ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    
                    # æ¬¡ã®å‡¦ç†ç¢ºèª
                    continue_choice = input("\nåˆ¥ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/n): ").strip().lower()
                    if continue_choice not in ['y', 'yes', 'ã¯ã„']:
                        break
                    continue
                
                # 2. ä¼æ¥­è©³ç´°è¡¨ç¤ºï¼ˆé€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼‰
                print(f"\n=== é¸æŠã•ã‚ŒãŸä¼æ¥­ ===")
                print(f"ä¼æ¥­å: {company_info['æå‡ºè€…å']}")
                print(f"EDINETã‚³ãƒ¼ãƒ‰: {company_info['EDINETã‚³ãƒ¼ãƒ‰']}")
                print(f"è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰: {company_info.get('è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰', 'ãªã—')}")
                print(f"æ±ºç®—æ—¥: {company_info.get('æ±ºç®—æ—¥', 'ä¸æ˜')}")
                print(f"æ¥­ç¨®: {company_info.get('æå‡ºè€…æ¥­ç¨®', 'ä¸æ˜')}")
                
                # 3. å‡¦ç†ãƒ¢ãƒ¼ãƒ‰é¸æŠ
                print(f"\n=== å‡¦ç†ãƒ¢ãƒ¼ãƒ‰é¸æŠ ===")
                print("1: æœ€æ–°ã®æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã®ã¿å–å¾—")
                print("2: æ—¥ä»˜ç¯„å›²ã‚’æŒ‡å®šã—ã¦è¤‡æ•°æ–‡æ›¸ã‚’çµ±åˆCSVå‡ºåŠ›")
                print("3: æ—¥ä»˜ç¯„å›²ã‚’æŒ‡å®šã—ã¦å„æ–‡æ›¸ã‚’å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›")
                print("0: ä¼æ¥­é¸æŠã«æˆ»ã‚‹")
                
                mode_choice = input("\nãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„ (1/2/3/0): ").strip()
                
                if mode_choice == '0':
                    continue
                elif mode_choice == '1':
                    # æœ€æ–°æ–‡æ›¸ã®ã¿å‡¦ç†
                    print("\n--- æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã‚’æ¤œç´¢ä¸­ ---")
                    documents = self.find_company_documents(company_info)
                    
                    if not documents:
                        print("æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                        continue
                    
                    latest_doc = documents[0]
                    print(f"\n=== ç™ºè¦‹ã•ã‚ŒãŸæ–‡æ›¸ ===")
                    print(f"æ–‡æ›¸: {latest_doc.get('docDescription')}")
                    print(f"æ–‡æ›¸ID: {latest_doc.get('docID')}")
                    print(f"æå‡ºæ—¥æ™‚: {latest_doc.get('submitDateTime')}")
                    print(f"å¯¾è±¡æœŸé–“: {latest_doc.get('periodStart')} ï½ {latest_doc.get('periodEnd')}")
                    
                    analyze = input("\nXBRLãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã¦è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¾ã™ã‹ï¼Ÿ (y/n): ").strip().lower()
                    
                    if analyze not in ['y', 'yes', 'ã¯ã„']:
                        continue
                    
                    print("\n--- XBRLãƒ•ã‚¡ã‚¤ãƒ«è§£æä¸­ ---")
                    print("â€» ã“ã®å‡¦ç†ã«ã¯æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™")
                    
                    result_file = self.process_company_document(company_info, latest_doc)
                    
                    if result_file:
                        print(f"\nâœ… è²¡å‹™ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†!")
                        print(f"çµæœãƒ•ã‚¡ã‚¤ãƒ«: {result_file}")
                        self.display_extraction_summary(result_file)
                    else:
                        print("âŒ è²¡å‹™ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                
                elif mode_choice == '2':
                    # æ—¥ä»˜ç¯„å›²æŒ‡å®šå‡¦ç†
                    print(f"\n=== æ—¥ä»˜ç¯„å›²æŒ‡å®š ===")
                    print("æ—¥ä»˜å½¢å¼: YYYY-MM-DDï¼ˆä¾‹: 2024-01-01ï¼‰")
                    
                    start_date = input("é–‹å§‹æ—¥ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
                    end_date = input("çµ‚äº†æ—¥ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
                    
                    if not start_date or not end_date:
                        print("æ—¥ä»˜ãŒå…¥åŠ›ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                        continue
                    
                    confirm = input(f"\næœŸé–“ {start_date} ï½ {end_date} ã§å‡¦ç†ã—ã¾ã™ã‹ï¼Ÿ (y/n): ").strip().lower()
                    
                    if confirm not in ['y', 'yes', 'ã¯ã„']:
                        continue
                    
                    print("\n--- æ—¥ä»˜ç¯„å›²ã§ã®æ–‡æ›¸å‡¦ç†é–‹å§‹ ---")
                    print("â€» æœŸé–“ãŒé•·ã„å ´åˆã¯å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
                    
                    result_files = self.process_date_range(company_info, start_date, end_date)
                    
                    if result_files:
                        print(f"\nâœ… æ—¥ä»˜ç¯„å›²å‡¦ç†å®Œäº†!")
                        print(f"å‡¦ç†ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(result_files)}")
                        for result_file in result_files:
                            print(f"  - {result_file}")
                    else:
                        print("âŒ å‡¦ç†ã§ããŸæ–‡æ›¸ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                
                elif mode_choice == '3':
                    # æ—¥ä»˜ç¯„å›²æŒ‡å®šå€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
                    print(f"\n=== æ—¥ä»˜ç¯„å›²æŒ‡å®šï¼ˆå€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ï¼‰ ===")
                    print("æ—¥ä»˜å½¢å¼: YYYY-MM-DDï¼ˆä¾‹: 2024-01-01ï¼‰")
                    print("â€» å„æ–‡æ›¸ãŒå€‹åˆ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ä¿å­˜ã•ã‚Œã¾ã™")
                    print("â€» CSVå‡ºåŠ›å…ˆ: ./output/csv/ ãƒ•ã‚©ãƒ«ãƒ€")
                    print("â€» JSONå‡ºåŠ›å…ˆ: ./output/json/ ãƒ•ã‚©ãƒ«ãƒ€")
                    print("â€» ãƒ•ã‚¡ã‚¤ãƒ«å: ä¼æ¥­å_è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰_EDINETã‚³ãƒ¼ãƒ‰_æ›¸é¡ç¨®åˆ¥_yyyymmdd")
                    
                    start_date = input("é–‹å§‹æ—¥ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
                    end_date = input("çµ‚äº†æ—¥ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
                    
                    if not start_date or not end_date:
                        print("æ—¥ä»˜ãŒå…¥åŠ›ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                        continue
                    
                    confirm = input(f"\næœŸé–“ {start_date} ï½ {end_date} ã§å„æ–‡æ›¸ã‚’å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã—ã¾ã™ã‹ï¼Ÿ (y/n): ").strip().lower()
                    
                    if confirm not in ['y', 'yes', 'ã¯ã„']:
                        continue
                    
                    print("\n--- æ—¥ä»˜ç¯„å›²ã§ã®å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹ ---")
                    print("â€» æœŸé–“ãŒé•·ã„å ´åˆã¯å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
                    print("â€» æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã¨å››åŠæœŸå ±å‘Šæ›¸ã‚’å–å¾—ã—ã¾ã™")
                    
                    result_files = self.process_date_range_individual(company_info, start_date, end_date)
                    
                    if result_files:
                        print(f"\nâœ… å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†!")
                        print(f"ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(result_files) * 2}ä»¶ï¼ˆCSV: {len(result_files)}ä»¶, JSON: {len(result_files)}ä»¶ï¼‰")
                        print("CSVå‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€: ./output/csv/")
                        print("JSONå‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€: ./output/json/")
                        print("ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
                        for result_file in result_files:
                            # ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿è¡¨ç¤º
                            json_filename = Path(result_file).name
                            csv_filename = json_filename.replace('.json', '.csv')
                            print(f"  - csv/{csv_filename}")
                            print(f"  - json/{json_filename}")
                    else:
                        print("âŒ å‡¦ç†ã§ããŸæ–‡æ›¸ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                
                else:
                    print("ç„¡åŠ¹ãªé¸æŠã§ã™ã€‚")
                    continue
                
                # ç¶™ç¶šç¢ºèª
                continue_choice = input("\nåˆ¥ã®ä¼æ¥­ã‚’å‡¦ç†ã—ã¾ã™ã‹ï¼Ÿ (y/n): ").strip().lower()
                if continue_choice not in ['y', 'yes', 'ã¯ã„']:
                    break
                    
            except KeyboardInterrupt:
                print("\n\nå‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚")
                break
            except Exception as e:
                logger.error(f"ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
                print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                continue_choice = input("\nåˆ¥ã®ä¼æ¥­ã‚’å‡¦ç†ã—ã¾ã™ã‹ï¼Ÿ (y/n): ").strip().lower()
                if continue_choice not in ['y', 'yes', 'ã¯ã„']:
                    break


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    try:
        extractor = FinancialDataExtractor()
        extractor.run_interactive_mode()
        
    except KeyboardInterrupt:
        print("\n\nå‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        logger.error(f"ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()